# See https://www.robotstxt.org/robotstxt.html for documentation on how to use the robots.txt file

# Allow all web crawlers (user-agents) to access all parts of the site
User-agent: *
# Disallow crawling access to all parts of the site (equivalent to Allow: /)
Disallow:
# Disallow crawling access to the '/users/sign_in' path for all web crawlers
Disallow: /users/sign_in